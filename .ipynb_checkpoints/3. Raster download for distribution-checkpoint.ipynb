{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import rasterio as rio\n",
    "import earthpy.spatial as es\n",
    "import pyproj;  \n",
    "\n",
    "import urllib\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "from ipypb import track\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## home directory for downloads\n",
    "os.chdir(\"E:/University College London/O'Sullivan, Aidan - SDG6/\")\n",
    "\n",
    "## path for where dsid csvs and sites3.p file are stored\n",
    "d_path = \"./Landsat data/Distributed download/\"\n",
    "\n",
    "## path where downloaded rasters to be saved\n",
    "c1_path = \"./Landsat data/Cropped level 1 data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropper(raster, geoms, outpath):\n",
    "    \"\"\"\n",
    "    This function accepts a raster object, an interable list of geometrys (or a single geometry),\n",
    "    and a filepath to save the cropped raster to, the cropped raster is then read back in and returned.\n",
    "    \"\"\"    \n",
    "    ## As crop accepts an iterable of geoms we first put any single geoms into a list\n",
    "    if not isinstance(geoms, Iterable):\n",
    "        geoms = [geoms]\n",
    "\n",
    "    ## Next we crop the image\n",
    "    raster_crop, raster_crop_meta = es.crop_image(raster, geoms)\n",
    "\n",
    "    ## We now need to update the metadata with the spatial data\n",
    "    raster_crop_meta.update({'transform': raster_crop_meta['transform'],\n",
    "                             'height': raster_crop.shape[1],\n",
    "                             'width': raster_crop.shape[2],\n",
    "                             'nodata': raster_crop.min()}) # <- This is the 'mask' value\n",
    "    \n",
    "    with rio.open(outpath, 'w', **raster_crop_meta) as file:\n",
    "        file.write(raster_crop[0], 1)\n",
    "        \n",
    "    raster_crop = rio.open(outpath)\n",
    "    \n",
    "    return raster_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in sites data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = pd.read_pickle(\"sites4.p\")#.set_index(['sid','dt'])\n",
    "sites = sites.set_geometry('geometry_poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter site data for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Filter sites and dates for those with scenes\n",
    "dll = sites[sites.display_id.notna()]\n",
    "\n",
    "## Filter sites for those with geometry\n",
    "dll = dll[dll.geometry_poly.notna()].set_geometry('geometry_poly')\n",
    "\n",
    "## Optional filter to remove sites with v large polygons\n",
    "dll = dll[dll.geometry_poly.area<4]\n",
    "\n",
    "len(dll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in list of scenes to download and filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of scenes to download: 901'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## change dsid file to read in\n",
    "dsids = list(pd.read_csv(d_path+'dsids_14.csv')['0'])\n",
    "\n",
    "## filter for for already downloaded scenes\n",
    "dowloaded_scenes = [ i.replace('_MTL.txt','') for i in os.listdir(c1_path) if 'MTL' in i ]\n",
    "\n",
    "dsids = [ i for i in dsids if i not in dowloaded_scenes ]\n",
    "\n",
    "## filter LM05 and LT08 scenes\n",
    "dsids = [ i for i in dsids if 'LM05' not in i ]\n",
    "dsids = [ i for i in dsids if 'LT08' not in i ]\n",
    "\n",
    "## filter for whether in updated sites data\n",
    "dll_dsids = dll.reset_index().display_id\n",
    "dll_dsids = [ i for i in dll_dsids.unique() if i is not np.nan ]\n",
    "\n",
    "dsids = [ i for i in dsids if i in dll_dsids ]\n",
    "\n",
    "\n",
    "f'Number of scenes to download: {len(dsids)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in exceptions lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## change directory for\n",
    "crop_exceptions = [pd.read_csv(d_path+'crop_exceptions.csv')]\n",
    "meta_exceptions = [pd.read_csv(d_path+'meta_exceptions.csv')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through scene list and sites to download and crop rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"901\" value=\"162\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>18%</strong></span>\n",
       "<span class=\"Iteration-label\">162/901</span>\n",
       "<span class=\"Time-label\">[02:06:28<00:36, 46.84s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [██████████##################################################] 162/901 [02:06:28<00:36, 46.84s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene meta not available LE07_L1GT_129208_20100709_20161213_01_T2\n",
      "Scene meta not available LE07_L1GT_129208_20100319_20161216_01_T2\n",
      "Scene meta not available LE07_L1GT_129208_20100911_20161212_01_T2\n",
      "Scene meta not available LE07_L1TP_031036_20120731_20160910_01_T1\n",
      "Scene meta not available LE07_L1TP_031036_20120731_20160910_01_T1\n",
      "Scene meta not available LE07_L1TP_030036_20130201_20160909_01_T1\n",
      "Scene meta not available LE07_L1GT_025037_20131012_20160907_01_T2\n",
      "Scene meta not available LE07_L1GT_024038_20171101_20171128_01_T2\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B1\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B1\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B2\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B2\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B3\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B3\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B4\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B4\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B5\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B5\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 BQA\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 BQA\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B6\n",
      "Crop exception for LT05_L1GS_026037_20090509_20160906_01_T2 B6\n",
      "Crop exception for LC08_L1GT_025037_20131121_20170307_01_T2 B2\n",
      "Crop exception for LC08_L1GT_025037_20131121_20170307_01_T2 B3\n",
      "Crop exception for LC08_L1GT_025037_20131121_20170307_01_T2 B4\n",
      "Crop exception for LC08_L1GT_025037_20131121_20170307_01_T2 B5\n",
      "Crop exception for LC08_L1GT_025037_20131121_20170307_01_T2 BQA\n",
      "Crop exception for LC08_L1GT_025037_20131121_20170307_01_T2 B10\n",
      "Crop exception for LC08_L1GT_025037_20131121_20170307_01_T2 B11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-15052eed5c49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'https://storage.googleapis.com/gcp-public-data-landsat/{platform}/01/{key}/{dsid}/{dsid}_{band}.TIF'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mrio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m## loop through sites with that scene\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\geo\\lib\\site-packages\\rasterio\\env.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\geo\\lib\\site-packages\\rasterio\\__init__.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# None.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_writer_for_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msharing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mrasterio\\_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\geo\\lib\\site-packages\\rasterio\\path.py\u001b[0m in \u001b[0;36mname\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mParsedPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;34m\"\"\"The parsed path's original URI\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## set crop buffer distance (300m)\n",
    "bbox_bufd = 300   # to calculate in degrees 360*300/(40000*1000)\n",
    "\n",
    "## create bands and metadata lists\n",
    "bands_5 = ['B1','B2','B3','B4','B5','BQA','B6']\n",
    "bands_7 = ['B1','B2','B3','B4','B5','BQA','B6_VCID_1','B6_VCID_2']\n",
    "bands_8 = ['B2','B3','B4','B5','BQA','B10','B11']\n",
    "\n",
    "meta_file_5 = ['MTL','ANG']\n",
    "meta_file_7 = ['MTL','ANG','GCP']\n",
    "meta_file_8 = ['MTL','ANG']\n",
    "\n",
    "for dsid in track(dsids):\n",
    "    \n",
    "    #~~~~ TIF download ~~~~#\n",
    "    \n",
    "    sitesdl = dll[dll.display_id==dsid]\n",
    "    \n",
    "    ## create iterable based on platform and bands\n",
    "    if 'LT05' in dsid:\n",
    "        zip_list = list(zip(bands_5,[dsid]*len(bands_5)))               \n",
    "    elif 'LE07' in dsid:\n",
    "        zip_list = list(zip(bands_7,[dsid]*len(bands_7)))\n",
    "    elif 'LC08' in dsid:\n",
    "        zip_list = list(zip(bands_8,[dsid]*len(bands_8)))\n",
    "    \n",
    "    ## open scene for each band in turn\n",
    "    for band,dsid in zip_list:\n",
    "\n",
    "        ## create url elements\n",
    "        platform = dsid[0:4]\n",
    "        key = dsid[10:13]+'/'+dsid[13:16]\n",
    "\n",
    "        ## TIF construct source url\n",
    "        filepath = f'https://storage.googleapis.com/gcp-public-data-landsat/{platform}/01/{key}/{dsid}/{dsid}_{band}.TIF'\n",
    "\n",
    "        with rio.open(filepath) as src:\n",
    "            \n",
    "            ## loop through sites with that scene\n",
    "            for sid in sitesdl.index.get_level_values(0):\n",
    "                \n",
    "                ## subset site list for site\n",
    "                sitedl = sitesdl.loc[sid,:]\n",
    "\n",
    "                ## setup polygon for cropping\n",
    "                polygon_bbox = sitedl.envelope.to_crs(src.crs) ## create bounding box and change to source crs                     \n",
    "                polygon_bbox = polygon_bbox.buffer(bbox_bufd) ## buffer bounding box by duffer distance e.g. 300m\n",
    "                polygon_geom = polygon_bbox.geometry          ## select geometry\n",
    "\n",
    "                try:\n",
    "\n",
    "                    cropped = cropper(src, polygon_geom, c1_path+sid+'__'+dsid+'_'+band+'.TIF')            \n",
    "\n",
    "                except:\n",
    "                    crop_exceptions.append(dsid+band)\n",
    "                    print(f'Crop exception for {dsid} {band}')\n",
    "\n",
    "\n",
    "            src.close()\n",
    "\n",
    "        time.sleep(1)\n",
    "        \n",
    "    #~~~~ metadata download ~~~~#\n",
    "    \n",
    "    ## create iterable based on platform and meta file\n",
    "    if 'LT05' in dsid:\n",
    "        mzip_list = list(zip(meta_file_5,[dsid]*len(meta_file_5)))               \n",
    "    elif 'LE07' in dsid:\n",
    "        mzip_list = list(zip(meta_file_7,[dsid]*len(meta_file_7)))  \n",
    "    elif 'LC08' in dsid:\n",
    "        mzip_list = list(zip(meta_file_8,[dsid]*len(meta_file_8)))  \n",
    "    \n",
    "    ## loop through meta dile and scenes\n",
    "    \n",
    "    for mf,dsid in mzip_list:\n",
    "\n",
    "        ## create url elements\n",
    "        platform = dsid[0:4]\n",
    "        key = dsid[10:13]+'/'+dsid[13:16]\n",
    "\n",
    "        ## Metadata construct source url\n",
    "        filepath = f'https://storage.googleapis.com/gcp-public-data-landsat/{platform}/01/{key}/{dsid}/{dsid}_{mf}.txt'\n",
    "\n",
    "        try:\n",
    "\n",
    "            ## MTL file\n",
    "            remote_file = urllib.request.urlopen(filepath).read()\n",
    "\n",
    "            local_file = open(c1_path+dsid+'_'+mf+'.txt','wb')\n",
    "            local_file.write(remote_file)\n",
    "            local_file.close()\n",
    "\n",
    "        except:\n",
    "            meta_exceptions.append(dsid)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            print(f'Scene meta not available {dsid}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write exceptions list to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(meta_exceptions).to_csv('./Landsat data/meta_exceptions.csv')\n",
    "pd.Series(crop_exceptions).to_csv('./Landsat data/crop_exceptions.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "732-392"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
